{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mebrahimkhani/miniconda3/envs/unet_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            question  \\\n",
      "0  A 23-year-old pregnant woman at 22 weeks gesta...   \n",
      "1  A 3-month-old baby died suddenly at night whil...   \n",
      "2  A mother brings her 3-week-old infant to the p...   \n",
      "3  A pulmonary autopsy specimen from a 58-year-ol...   \n",
      "4  A 20-year-old woman presents with menorrhagia ...   \n",
      "\n",
      "                                             options  \\\n",
      "0  ['Ampicillin', 'Ceftriaxone', 'Doxycycline', '...   \n",
      "1  ['Placing the infant in a supine position on a...   \n",
      "2  ['Abnormal migration of ventral pancreatic bud...   \n",
      "3  ['Thromboembolism', 'Pulmonary ischemia', 'Pul...   \n",
      "4  ['Hemophilia A', 'Lupus anticoagulant', 'Prote...   \n",
      "\n",
      "                                              answer  \\\n",
      "0                                     Nitrofurantoin   \n",
      "1  Placing the infant in a supine position on a f...   \n",
      "2       Abnormal migration of ventral pancreatic bud   \n",
      "3                                    Thromboembolism   \n",
      "4                             Von Willebrand disease   \n",
      "\n",
      "                                             context  \n",
      "0  Ampicillin Ceftriaxone Doxycycline Nitrofurantoin  \n",
      "1  Placing the infant in a supine position on a f...  \n",
      "2  Abnormal migration of ventral pancreatic bud C...  \n",
      "3  Thromboembolism Pulmonary ischemia Pulmonary h...  \n",
      "4  Hemophilia A Lupus anticoagulant Protein C def...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "def preprocess_medqa(df):\n",
    "    \"\"\" Prepares dataset for retrieval-based QA. \"\"\"\n",
    "    processed_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        question = row[\"sent1\"]\n",
    "        # Four multiple-choice options\n",
    "        options = [row[f\"ending{i}\"] for i in range(4)]\n",
    "        # 'label' indicates which option is correct\n",
    "        correct_answer = options[row[\"label\"]]\n",
    "\n",
    "        processed_data.append({\n",
    "            \"question\": question,\n",
    "            \"options\": options,\n",
    "            \"answer\": correct_answer,\n",
    "            # For real RAG, you might fetch relevant doc passages or knowledge base entries. \n",
    "            # Here we use the 4 options as \"context\" for demonstration.\n",
    "            \"context\": \" \".join(options)  \n",
    "        })\n",
    "    return pd.DataFrame(processed_data)\n",
    "\n",
    "# Check if processed files already exist\n",
    "if (os.path.exists(\"medical_train.csv\") and \n",
    "    os.path.exists(\"medical_val.csv\") and \n",
    "    os.path.exists(\"medical_test.csv\")):\n",
    "    \n",
    "    # Load existing processed files\n",
    "    df_train_proc = pd.read_csv(\"medical_train.csv\")\n",
    "    df_val_proc = pd.read_csv(\"medical_val.csv\") \n",
    "    df_test_proc = pd.read_csv(\"medical_test.csv\")\n",
    "    \n",
    "else:\n",
    "    # Load and process MedQA-USMLE dataset\n",
    "    dataset = load_dataset(\"GBaker/MedQA-USMLE-4-options-hf\")\n",
    "\n",
    "    df_train = pd.DataFrame(dataset[\"train\"])\n",
    "    df_val = pd.DataFrame(dataset[\"validation\"])\n",
    "    df_test = pd.DataFrame(dataset[\"test\"])\n",
    "\n",
    "    df_train_proc = preprocess_medqa(df_train)\n",
    "    df_val_proc = preprocess_medqa(df_val)\n",
    "    df_test_proc = preprocess_medqa(df_test)\n",
    "\n",
    "    # Save processed datasets\n",
    "    df_train_proc.to_csv(\"medical_train.csv\", index=False)\n",
    "    df_val_proc.to_csv(\"medical_val.csv\", index=False)\n",
    "    df_test_proc.to_csv(\"medical_test.csv\", index=False)\n",
    "\n",
    "print(df_train_proc.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create medical knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/mambaforge/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new knowledge base and index...\n",
      "Loading datasets...\n",
      "\n",
      "Example from PubMedQA:\n",
      "{'context_preview': \"{'contexts': ['Programmed cell death (PCD) is the \"\n",
      "                    'regulated death of cells within an organism. The lace '\n",
      "                    'plant (Aponogeton madagascariensis) produces perforations '\n",
      "                    'in its leaves through PCD. The leaves ...',\n",
      " 'long_answer': 'Results depicted mitochondrial dynamics in vivo as PCD '\n",
      "                'progresses within the lace plant, and highlight the '\n",
      "                'correlation of this organelle with other organelles during '\n",
      "                'developmental PCD. To the best of our knowledge, this is the '\n",
      "                'first report of mitochondria and chloroplasts moving on '\n",
      "                'transvacuolar strands to form a ring structure surrounding '\n",
      "                'the nucleus during developmental PCD. Also, for the first '\n",
      "                'time, we have shown the feasibility for the use of CsA in a '\n",
      "                'whole plant system. Overall, our findings implicate the '\n",
      "                'mitochondria as playing a critical and early role in '\n",
      "                'developmentally regulated PCD in the lace plant.',\n",
      " 'question': 'Do mitochondria play a role in remodelling lace plant leaves '\n",
      "             'during programmed cell death?'}\n",
      "\n",
      "Example from MedMCQA:\n",
      "{'correct_option': 2,\n",
      " 'explanation_preview': 'Chronic urethral obstruction because of urinary '\n",
      "                        'calculi, prostatic hyperophy, tumors, normal '\n",
      "                        'pregnancy, tumors, uterine prolapse or functional '\n",
      "                        'disorders cause hydronephrosis which by definition is '\n",
      "                        'use...',\n",
      " 'question': 'Chronic urethral obstruction due to benign prismatic hyperplasia '\n",
      "             'can lead to the following change in kidney parenchyma'}\n",
      "\n",
      "Created knowledge base with 129952 entries\n",
      "\n",
      "Available SentenceTransformer Models:\n",
      "\n",
      "pritamdeka/S-PubMedBert-MS-MARCO:\n",
      "Parameters: 110M\n",
      "Embedding Dimension: 768\n",
      "Description: Specialized for medical/biomedical text, fine-tuned on PubMed\n",
      "\n",
      "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext:\n",
      "Parameters: 110M\n",
      "Embedding Dimension: 768\n",
      "Description: Trained on PubMed abstracts and full-text articles\n",
      "\n",
      "gsarti/biobert-nli:\n",
      "Parameters: 110M\n",
      "Embedding Dimension: 768\n",
      "Description: BioBERT fine-tuned on NLI tasks, good for medical similarity\n",
      "\n",
      "all-MiniLM-L6-v2:\n",
      "Parameters: 22M\n",
      "Embedding Dimension: 384\n",
      "Description: Fast and lightweight model, good balance of speed and performance\n",
      "\n",
      "all-mpnet-base-v2:\n",
      "Parameters: 110M\n",
      "Embedding Dimension: 768\n",
      "Description: One of the best performing general models\n",
      "Using default model: pritamdeka/S-PubMedBert-MS-MARCO\n",
      "\n",
      "Loading pritamdeka/S-PubMedBert-MS-MARCO...\n",
      "Found 4 CUDA GPUs\n",
      "Using 4 GPUs in parallel\n",
      "Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [07:44<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating embeddings...\n",
      "\n",
      "Available FAISS Index Types:\n",
      "\n",
      "IndexFlatL2:\n",
      "Description: Exact L2 distance search. Most accurate but slower for large datasets.\n",
      "Use Case: Small to medium datasets where accuracy is critical\n",
      "Recommended Dataset Size: < 1M vectors\n",
      "\n",
      "IndexIVFFlat:\n",
      "Description: Inverted file with exact post-verification. Good balance of speed and accuracy.\n",
      "Use Case: Medium to large datasets, allows approximate search\n",
      "Recommended Dataset Size: 1M - 10M vectors\n",
      "\n",
      "IndexHNSWFlat:\n",
      "Description: Hierarchical Navigable Small World graph. Very fast search with good accuracy.\n",
      "Use Case: Large datasets where search speed is critical\n",
      "Recommended Dataset Size: 10M - 100M vectors\n",
      "\n",
      "IndexLSH:\n",
      "Description: Locality-Sensitive Hashing. Fast but less accurate.\n",
      "Use Case: Very large datasets where approximate results are acceptable\n",
      "Recommended Dataset Size: > 100M vectors\n",
      "\n",
      "Based on your dataset size (129,952 vectors), we recommend using: IndexFlatL2\n",
      "Adding vectors to index...\n",
      "Saving knowledge base, index and model name...\n",
      "\n",
      "Testing retrieval system with example queries:\n",
      "\n",
      "Query: What are the symptoms of diabetes?\n",
      "Error retrieving contexts: 'DataParallel' object has no attribute 'encode'\n",
      "\n",
      "Query: How is breast cancer diagnosed?\n",
      "Error retrieving contexts: 'DataParallel' object has no attribute 'encode'\n",
      "\n",
      "Query: What are the side effects of chemotherapy?\n",
      "Error retrieving contexts: 'DataParallel' object has no attribute 'encode'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Define available models with their characteristics\n",
    "AVAILABLE_MODELS = {\n",
    "    \"pritamdeka/S-PubMedBert-MS-MARCO\": {\n",
    "        \"parameters\": \"110M\",\n",
    "        \"embedding_dim\": 768,\n",
    "        \"description\": \"Specialized for medical/biomedical text, fine-tuned on PubMed\"\n",
    "    },\n",
    "    \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\": {\n",
    "        \"parameters\": \"110M\",\n",
    "        \"embedding_dim\": 768,\n",
    "        \"description\": \"Trained on PubMed abstracts and full-text articles\"\n",
    "    },\n",
    "    \"gsarti/biobert-nli\": {\n",
    "        \"parameters\": \"110M\",\n",
    "        \"embedding_dim\": 768,\n",
    "        \"description\": \"BioBERT fine-tuned on NLI tasks, good for medical similarity\"\n",
    "    },\n",
    "    \"all-MiniLM-L6-v2\": {\n",
    "        \"parameters\": \"22M\",\n",
    "        \"embedding_dim\": 384,\n",
    "        \"description\": \"Fast and lightweight model, good balance of speed and performance\"\n",
    "    },\n",
    "    \"all-mpnet-base-v2\": {\n",
    "        \"parameters\": \"110M\", \n",
    "        \"embedding_dim\": 768,\n",
    "        \"description\": \"One of the best performing general models\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define available FAISS index types\n",
    "FAISS_INDEXES = {\n",
    "    \"IndexFlatL2\": {\n",
    "        \"description\": \"Exact L2 distance search. Most accurate but slower for large datasets.\",\n",
    "        \"use_case\": \"Small to medium datasets where accuracy is critical\",\n",
    "        \"recommended_size\": \"< 1M vectors\"\n",
    "    },\n",
    "    \"IndexIVFFlat\": {\n",
    "        \"description\": \"Inverted file with exact post-verification. Good balance of speed and accuracy.\",\n",
    "        \"use_case\": \"Medium to large datasets, allows approximate search\",\n",
    "        \"recommended_size\": \"1M - 10M vectors\"\n",
    "    },\n",
    "    \"IndexHNSWFlat\": {\n",
    "        \"description\": \"Hierarchical Navigable Small World graph. Very fast search with good accuracy.\",\n",
    "        \"use_case\": \"Large datasets where search speed is critical\",\n",
    "        \"recommended_size\": \"10M - 100M vectors\"\n",
    "    },\n",
    "    \"IndexLSH\": {\n",
    "        \"description\": \"Locality-Sensitive Hashing. Fast but less accurate.\",\n",
    "        \"use_case\": \"Very large datasets where approximate results are acceptable\",\n",
    "        \"recommended_size\": \"> 100M vectors\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def create_medical_knowledge_base():\n",
    "    \"\"\"Create a comprehensive medical knowledge base from multiple sources with quality filtering\"\"\"\n",
    "    knowledge_base = []\n",
    "    \n",
    "    print(\"Loading datasets...\")\n",
    "    pubmedqa = load_dataset(\"pubmed_qa\", \"pqa_labeled\")\n",
    "    medmcqa = load_dataset(\"medmcqa\")\n",
    "    \n",
    "    # Show some examples\n",
    "    print(\"\\nExample from PubMedQA:\")\n",
    "    example_pubmed = pubmedqa['train'][0]\n",
    "    # Convert context to string before slicing\n",
    "    context_preview = str(example_pubmed['context'])[:200] + \"...\"\n",
    "    pprint({\n",
    "        'question': example_pubmed['question'],\n",
    "        'context_preview': context_preview,\n",
    "        'long_answer': example_pubmed['long_answer']\n",
    "    })\n",
    "    \n",
    "    print(\"\\nExample from MedMCQA:\")\n",
    "    example_medmcqa = medmcqa['train'][0]\n",
    "    # Convert explanation to string before slicing\n",
    "    exp_preview = str(example_medmcqa['exp'])[:200] + \"...\" if example_medmcqa['exp'] else \"No explanation\"\n",
    "    pprint({\n",
    "        'question': example_medmcqa['question'],\n",
    "        'explanation_preview': exp_preview,\n",
    "        'correct_option': example_medmcqa['cop']\n",
    "    })\n",
    "    \n",
    "    # Add PubMedQA abstracts\n",
    "    for item in pubmedqa['train']:\n",
    "        # Join all context pieces into a single string\n",
    "        context_text = \" \".join(item['context']['contexts'])\n",
    "        if len(context_text.split()) >= 20:  # Length check\n",
    "            knowledge_base.append({\n",
    "                'text': context_text,\n",
    "                'source': 'PubMedQA',\n",
    "                'type': 'research_abstract',\n",
    "                'metadata': {\n",
    "                    'question': item['question'],\n",
    "                    'long_answer': item['long_answer'],\n",
    "                    'pubid': item['pubid']\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    # Add MedMCQA explanations\n",
    "    for item in medmcqa['train']:\n",
    "        if item['exp'] and len(item['exp'].split()) >= 20:  # Check if explanation exists and length\n",
    "            knowledge_base.append({\n",
    "                'text': item['exp'],\n",
    "                'source': 'MedMCQA', \n",
    "                'type': 'expert_explanation',\n",
    "                'metadata': {\n",
    "                    'question': item['question'],\n",
    "                    'correct_answer': item['cop']\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    print(f\"\\nCreated knowledge base with {len(knowledge_base)} entries\")\n",
    "    return knowledge_base\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]\n",
    "\n",
    "def build_retrieval_system(knowledge_base):\n",
    "    \"\"\"Build dense retrieval system with FAISS index\"\"\"\n",
    "     # Print available models and their info\n",
    "    print(\"\\nAvailable SentenceTransformer Models:\")\n",
    "    for model_name, info in AVAILABLE_MODELS.items():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"Parameters: {info['parameters']}\")\n",
    "        print(f\"Embedding Dimension: {info['embedding_dim']}\")\n",
    "        print(f\"Description: {info['description']}\")\n",
    "\n",
    "    # Model selection\n",
    "    selected_model = input(\"\\nEnter the name of the model you want to use (default: pritamdeka/S-PubMedBert-MS-MARCO): \").strip()\n",
    "    if not selected_model or selected_model not in AVAILABLE_MODELS:\n",
    "        print(f\"Using default model: pritamdeka/S-PubMedBert-MS-MARCO\")\n",
    "        selected_model = \"pritamdeka/S-PubMedBert-MS-MARCO\"\n",
    "\n",
    "    # Initialize encoder and configure GPU usage\n",
    "    print(f\"\\nLoading {selected_model}...\")\n",
    "    encoder = SentenceTransformer(selected_model)\n",
    "\n",
    "    # Enhanced GPU detection and configuration\n",
    "    if torch.cuda.is_available():\n",
    "        n_gpus = torch.cuda.device_count()\n",
    "        print(f\"Found {n_gpus} CUDA GPUs\")\n",
    "        \n",
    "        if n_gpus > 1:\n",
    "            print(f\"Using {n_gpus} GPUs in parallel\")\n",
    "            # Use DataParallel with all available GPUs\n",
    "            encoder = torch.nn.DataParallel(encoder)\n",
    "            # Scale batch size with number of GPUs, but cap it for stability\n",
    "            batch_size = min(32 * n_gpus, 256)  # Cap at 256 to prevent OOM\n",
    "        else:\n",
    "            print(\"Using single GPU\")\n",
    "            device = torch.device(\"cuda:0\")\n",
    "            encoder.to(device)\n",
    "            batch_size = 64\n",
    "    else:\n",
    "        print(\"CUDA is not available. Using CPU.\")\n",
    "        device = torch.device(\"cpu\")\n",
    "        batch_size = 32\n",
    "\n",
    "    # Optimize data loading for multi-GPU\n",
    "    texts = [entry['text'] for entry in knowledge_base]\n",
    "    dataset = TextDataset(texts)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=min(4, os.cpu_count() or 1),\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        persistent_workers=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "\n",
    "    # Optimized embedding generation\n",
    "    print(\"Generating embeddings...\")\n",
    "    embeddings_list = []\n",
    "    encoder.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            try:\n",
    "                if isinstance(encoder, torch.nn.DataParallel):\n",
    "                    # Automatic batch splitting across GPUs\n",
    "                    batch_embeddings = encoder.module.encode(\n",
    "                        batch,\n",
    "                        convert_to_numpy=True,\n",
    "                        device=None  # Let DataParallel handle device placement\n",
    "                    )\n",
    "                else:\n",
    "                    # Single GPU or CPU processing\n",
    "                    batch_embeddings = encoder.encode(\n",
    "                        batch,\n",
    "                        convert_to_numpy=True,\n",
    "                        device=device if 'device' in locals() else None\n",
    "                    )\n",
    "                \n",
    "                embeddings_list.append(batch_embeddings)\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error processing batch: {e}\")\n",
    "                # Graceful fallback with reduced batch size\n",
    "                if len(batch) > 1:\n",
    "                    print(\"Reducing batch size and retrying...\")\n",
    "                    # Process in smaller chunks\n",
    "                    chunk_size = len(batch) // 4\n",
    "                    for i in range(0, len(batch), chunk_size):\n",
    "                        sub_batch = batch[i:i + chunk_size]\n",
    "                        if isinstance(encoder, torch.nn.DataParallel):\n",
    "                            sub_embeddings = encoder.module.encode(\n",
    "                                sub_batch,\n",
    "                                convert_to_numpy=True,\n",
    "                                device=None\n",
    "                            )\n",
    "                        else:\n",
    "                            sub_embeddings = encoder.encode(\n",
    "                                sub_batch,\n",
    "                                convert_to_numpy=True,\n",
    "                                device=device if 'device' in locals() else None\n",
    "                            )\n",
    "                        embeddings_list.append(sub_embeddings)\n",
    "                else:\n",
    "                    raise e\n",
    "\n",
    "            # Explicit GPU memory cleanup\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    # Concatenate embeddings\n",
    "    print(\"Concatenating embeddings...\")\n",
    "    embeddings = np.concatenate(embeddings_list, axis=0)\n",
    "    \n",
    "    # FAISS index selection and creation\n",
    "    print(\"\\nAvailable FAISS Index Types:\")\n",
    "    for index_name, info in FAISS_INDEXES.items():\n",
    "        print(f\"\\n{index_name}:\")\n",
    "        print(f\"Description: {info['description']}\")\n",
    "        print(f\"Use Case: {info['use_case']}\")\n",
    "        print(f\"Recommended Dataset Size: {info['recommended_size']}\")\n",
    "\n",
    "    data_size = len(embeddings)\n",
    "    recommended_index = \"IndexFlatL2\"\n",
    "    if data_size > 100_000_000:\n",
    "        recommended_index = \"IndexLSH\"\n",
    "    elif data_size > 10_000_000:\n",
    "        recommended_index = \"IndexHNSWFlat\"\n",
    "    elif data_size > 1_000_000:\n",
    "        recommended_index = \"IndexIVFFlat\"\n",
    "\n",
    "    print(f\"\\nBased on your dataset size ({data_size:,} vectors), we recommend using: {recommended_index}\")\n",
    "    \n",
    "    selected_index = input(\"\\nEnter the name of the index type you want to use (default: recommended): \").strip()\n",
    "    if not selected_index or selected_index not in FAISS_INDEXES:\n",
    "        print(f\"Using recommended index: {recommended_index}\")\n",
    "        selected_index = recommended_index\n",
    "\n",
    "    # Build FAISS index\n",
    "    dimension = embeddings.shape[1]\n",
    "    \n",
    "    try:\n",
    "        if selected_index == \"IndexFlatL2\":\n",
    "            index = faiss.IndexFlatL2(dimension)\n",
    "        elif selected_index == \"IndexIVFFlat\":\n",
    "            nlist = min(4096, max(data_size // 30, 100))\n",
    "            quantizer = faiss.IndexFlatL2(dimension)\n",
    "            index = faiss.IndexIVFFlat(quantizer, dimension, nlist)\n",
    "            print(\"Training IVF index...\")\n",
    "            index.train(embeddings)\n",
    "        elif selected_index == \"IndexHNSWFlat\":\n",
    "            M = 32\n",
    "            index = faiss.IndexHNSWFlat(dimension, M)\n",
    "        elif selected_index == \"IndexLSH\":\n",
    "            nbits = min(64, dimension)\n",
    "            index = faiss.IndexLSH(dimension, nbits)\n",
    "\n",
    "        print(\"Adding vectors to index...\")\n",
    "        index.add(embeddings)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating FAISS index: {e}\")\n",
    "        print(\"Falling back to simple IndexFlatL2...\")\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "        index.add(embeddings)\n",
    "    \n",
    "    return index, encoder, selected_model\n",
    "\n",
    "def retrieve_contexts(query, index, encoder, knowledge_base, k=3):\n",
    "    \"\"\"Retrieve relevant contexts for a query\"\"\"\n",
    "    try:\n",
    "        # Handle both DataParallel and regular encoder cases\n",
    "        if isinstance(encoder, torch.nn.DataParallel):\n",
    "            actual_encoder = encoder.module\n",
    "        else:\n",
    "            actual_encoder = encoder\n",
    "            \n",
    "        # Move query to same device as encoder\n",
    "        device = next(actual_encoder.parameters()).device\n",
    "        \n",
    "        # Encode query\n",
    "        with torch.no_grad():\n",
    "            query_vector = actual_encoder.encode([query], convert_to_numpy=True, device=device)\n",
    "        \n",
    "        # Search index\n",
    "        distances, indices = index.search(query_vector, k)\n",
    "        \n",
    "        # Return relevant contexts with metadata and distances\n",
    "        retrieved = []\n",
    "        for idx, distance in zip(indices[0], distances[0]):\n",
    "            if 0 <= idx < len(knowledge_base):  # Validate index\n",
    "                context = knowledge_base[idx].copy()\n",
    "                context['distance'] = float(distance)\n",
    "                retrieved.append(context)\n",
    "        \n",
    "        return retrieved\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving contexts: {e}\")\n",
    "        return []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    kb_file = \"medical_knowledge_base.pkl\"\n",
    "    index_file = \"faiss_index.bin\"\n",
    "    model_name_file = \"model_name.txt\"\n",
    "    \n",
    "    try:\n",
    "        if os.path.exists(kb_file) and os.path.exists(index_file) and os.path.exists(model_name_file):\n",
    "            print(\"Loading existing knowledge base and index...\")\n",
    "            with open(kb_file, 'rb') as f:\n",
    "                kb = pickle.load(f)\n",
    "            index = faiss.read_index(index_file)\n",
    "            with open(model_name_file, 'r') as f:\n",
    "                model_name = f.read().strip()\n",
    "            encoder = SentenceTransformer(model_name)\n",
    "        else:\n",
    "            print(\"Creating new knowledge base and index...\")\n",
    "            kb = create_medical_knowledge_base()\n",
    "            index, encoder, model_name = build_retrieval_system(kb)\n",
    "            \n",
    "            print(\"Saving knowledge base, index and model name...\")\n",
    "            with open(kb_file, 'wb') as f:\n",
    "                pickle.dump(kb, f)\n",
    "            faiss.write_index(index, index_file)\n",
    "            with open(model_name_file, 'w') as f:\n",
    "                f.write(model_name)\n",
    "        \n",
    "        # Example queries\n",
    "        example_queries = [\n",
    "            \"What are the symptoms of diabetes?\",\n",
    "            \"How is breast cancer diagnosed?\",\n",
    "            \"What are the side effects of chemotherapy?\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nTesting retrieval system with example queries:\")\n",
    "        for query in example_queries:\n",
    "            print(f\"\\nQuery: {query}\")\n",
    "            relevant_contexts = retrieve_contexts(query, index, encoder, kb)\n",
    "            for i, context in enumerate(relevant_contexts, 1):\n",
    "                print(f\"\\nRelevant Context {i}:\")\n",
    "                print(f\"Source: {context['source']}\")\n",
    "                print(f\"Type: {context['type']}\")\n",
    "                print(f\"Text preview: {context['text'][:200]}...\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Small LLM for RAG: Flan-T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS backend for Apple Silicon.\n",
      "Provided context:\n",
      " Decreased vascular resistance Increased cardiac output Diastolic murmur Low blood pressure\n",
      "Generated Asnwer: Decreased vascular resistance\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS backend for Apple Silicon.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU.\")\n",
    "model.to(device)\n",
    "\n",
    "# RAG inference pipeline\n",
    "def rag_infer(question, top_k=1, max_length=128):\n",
    "    # Retrieve context\n",
    "    retrieved_list = retrieve_context(question, k=top_k)\n",
    "    combined_context = \"\\n\".join(retrieved_list)\n",
    "    \n",
    "    # Prepare prompt for T5\n",
    "    prompt = f\"answer the medical question based on context:\\nContext: {combined_context}\\nQuestion: {question}\\nAnswer:\"\n",
    "    # prompt = f\"answer the medical question based on context:\\nQuestion: {question}\\nAnswer:\"\n",
    "    \n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return combined_context, answer\n",
    "    \n",
    "# Quick test\n",
    "context, sample_answer = rag_infer(\"What is the cause of low blood pressure?\")\n",
    "print(f\"Provided context:\\n {context}\")\n",
    "print(f\"Generated Asnwer: {sample_answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning Flan-T5 with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10178/10178 [00:03<00:00, 2731.16 examples/s]\n",
      "Map: 100%|██████████| 1272/1272 [00:00<00:00, 2980.02 examples/s]\n",
      "/var/folders/7m/znbybd0d4dq84vh8my014q3c0000gn/T/ipykernel_50853/2357161580.py:59: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 8.42 GB, other allocations: 11.98 GB, max allowed: 20.40 GB). Tried to allocate 741.00 KB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 68\u001b[0m\n\u001b[1;32m     46\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     47\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./flan_t5_medical\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     48\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m     push_to_hub\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     57\u001b[0m )\n\u001b[1;32m     59\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     60\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     61\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 68\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unet_env/lib/python3.10/site-packages/transformers/trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unet_env/lib/python3.10/site-packages/transformers/trainer.py:2531\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2524\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2525\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2529\u001b[0m )\n\u001b[1;32m   2530\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2531\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2534\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2535\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2536\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2537\u001b[0m ):\n\u001b[1;32m   2538\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/unet_env/lib/python3.10/site-packages/transformers/trainer.py:3675\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3674\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3675\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3677\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3679\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3680\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3681\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/unet_env/lib/python3.10/site-packages/transformers/trainer.py:3731\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3729\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3730\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3731\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3732\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3733\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/unet_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unet_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/unet_env/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1854\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1851\u001b[0m \u001b[38;5;66;03m# Encode if needed (training, first prediction pass)\u001b[39;00m\n\u001b[1;32m   1852\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1853\u001b[0m     \u001b[38;5;66;03m# Convert encoder inputs in embeddings if needed\u001b[39;00m\n\u001b[0;32m-> 1854\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1855\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1857\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1858\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1859\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n\u001b[1;32m   1864\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[1;32m   1865\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1866\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1867\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1868\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/unet_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unet_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/unet_env/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1124\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1107\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1108\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[1;32m   1109\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1121\u001b[0m         cache_position,\n\u001b[1;32m   1122\u001b[0m     )\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1124\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/unet_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unet_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/unet_env/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:675\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    661\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    673\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    674\u001b[0m ):\n\u001b[0;32m--> 675\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    685\u001b[0m     hidden_states, past_key_value \u001b[38;5;241m=\u001b[39m self_attention_outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    686\u001b[0m     attention_outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/unet_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unet_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/unet_env/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:592\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    583\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    590\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    591\u001b[0m ):\n\u001b[0;32m--> 592\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSelfAttention(\n\u001b[1;32m    594\u001b[0m         normed_hidden_states,\n\u001b[1;32m    595\u001b[0m         mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    601\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    602\u001b[0m     )\n\u001b[1;32m    603\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/unet_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unet_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/unet_env/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:249\u001b[0m, in \u001b[0;36mT5LayerNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;66;03m# T5 uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# Square Layer Normalization https://arxiv.org/abs/1910.07467 thus varience is calculated\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;66;03m# w/o mean and there is no bias. Additionally we want to make sure that the accumulation for\u001b[39;00m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;66;03m# half-precision inputs is done in fp32\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m     variance \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    250\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrsqrt(variance \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariance_epsilon)\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;66;03m# convert into half-precision if necessary\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 8.42 GB, other allocations: 11.98 GB, max allowed: 20.40 GB). Tried to allocate 741.00 KB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "def prepare_finetune_data(df):\n",
    "    data_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        # input: RAG style prompt\n",
    "        input_text = (\n",
    "            f\"answer the medical question based on context:\\n\"\n",
    "            f\"Context: {row['context']}\\n\"\n",
    "            f\"Question: {row['question']}\\n\"\n",
    "            f\"Answer:\"\n",
    "        )\n",
    "        # target: correct answer\n",
    "        target_text = row[\"answer\"]\n",
    "        data_list.append({\"input_text\": input_text, \"target_text\": target_text})\n",
    "    return pd.DataFrame(data_list)\n",
    "\n",
    "ft_train = prepare_finetune_data(df_train_proc)\n",
    "ft_val = prepare_finetune_data(df_val_proc)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(ft_train)\n",
    "val_dataset = Dataset.from_pandas(ft_val)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    model_inputs = tokenizer(example[\"input_text\"], max_length=512, truncation=True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(example[\"target_text\"], max_length=128, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True,\n",
    "    max_length=512,\n",
    "    label_pad_token_id=-100,\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "valid_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "train_dataset = train_dataset.remove_columns([\"input_text\", \"target_text\"])\n",
    "val_dataset = val_dataset.remove_columns([\"input_text\", \"target_text\"])\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./flan_t5_medical\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs = 1,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer, \n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
