{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mebrahimkhani/miniconda3/envs/unet_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            question  \\\n",
      "0  A 23-year-old pregnant woman at 22 weeks gesta...   \n",
      "1  A 3-month-old baby died suddenly at night whil...   \n",
      "2  A mother brings her 3-week-old infant to the p...   \n",
      "3  A pulmonary autopsy specimen from a 58-year-ol...   \n",
      "4  A 20-year-old woman presents with menorrhagia ...   \n",
      "\n",
      "                                             options  \\\n",
      "0  ['Ampicillin', 'Ceftriaxone', 'Doxycycline', '...   \n",
      "1  ['Placing the infant in a supine position on a...   \n",
      "2  ['Abnormal migration of ventral pancreatic bud...   \n",
      "3  ['Thromboembolism', 'Pulmonary ischemia', 'Pul...   \n",
      "4  ['Hemophilia A', 'Lupus anticoagulant', 'Prote...   \n",
      "\n",
      "                                              answer  \\\n",
      "0                                     Nitrofurantoin   \n",
      "1  Placing the infant in a supine position on a f...   \n",
      "2       Abnormal migration of ventral pancreatic bud   \n",
      "3                                    Thromboembolism   \n",
      "4                             Von Willebrand disease   \n",
      "\n",
      "                                             context  \n",
      "0  Ampicillin Ceftriaxone Doxycycline Nitrofurantoin  \n",
      "1  Placing the infant in a supine position on a f...  \n",
      "2  Abnormal migration of ventral pancreatic bud C...  \n",
      "3  Thromboembolism Pulmonary ischemia Pulmonary h...  \n",
      "4  Hemophilia A Lupus anticoagulant Protein C def...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "def preprocess_medqa(df):\n",
    "    \"\"\" Prepares dataset for retrieval-based QA. \"\"\"\n",
    "    processed_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        question = row[\"sent1\"]\n",
    "        # Four multiple-choice options\n",
    "        options = [row[f\"ending{i}\"] for i in range(4)]\n",
    "        # 'label' indicates which option is correct\n",
    "        correct_answer = options[row[\"label\"]]\n",
    "\n",
    "        processed_data.append({\n",
    "            \"question\": question,\n",
    "            \"options\": options,\n",
    "            \"answer\": correct_answer,\n",
    "            # For real RAG, you might fetch relevant doc passages or knowledge base entries. \n",
    "            # Here we use the 4 options as \"context\" for demonstration.\n",
    "            \"context\": \" \".join(options)  \n",
    "        })\n",
    "    return pd.DataFrame(processed_data)\n",
    "\n",
    "# Check if processed files already exist\n",
    "if (os.path.exists(\"medical_train.csv\") and \n",
    "    os.path.exists(\"medical_val.csv\") and \n",
    "    os.path.exists(\"medical_test.csv\")):\n",
    "    \n",
    "    # Load existing processed files\n",
    "    df_train_proc = pd.read_csv(\"medical_train.csv\")\n",
    "    df_val_proc = pd.read_csv(\"medical_val.csv\") \n",
    "    df_test_proc = pd.read_csv(\"medical_test.csv\")\n",
    "    \n",
    "else:\n",
    "    # Load and process MedQA-USMLE dataset\n",
    "    dataset = load_dataset(\"GBaker/MedQA-USMLE-4-options-hf\")\n",
    "\n",
    "    df_train = pd.DataFrame(dataset[\"train\"])\n",
    "    df_val = pd.DataFrame(dataset[\"validation\"])\n",
    "    df_test = pd.DataFrame(dataset[\"test\"])\n",
    "\n",
    "    df_train_proc = preprocess_medqa(df_train)\n",
    "    df_val_proc = preprocess_medqa(df_val)\n",
    "    df_test_proc = preprocess_medqa(df_test)\n",
    "\n",
    "    # Save processed datasets\n",
    "    df_train_proc.to_csv(\"medical_train.csv\", index=False)\n",
    "    df_val_proc.to_csv(\"medical_val.csv\", index=False)\n",
    "    df_test_proc.to_csv(\"medical_test.csv\", index=False)\n",
    "\n",
    "print(df_train_proc.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create medical knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing knowledge base and index...\n",
      "\n",
      "Testing retrieval system with example queries:\n",
      "\n",
      "Query: What are the symptoms of diabetes?\n",
      "\n",
      "Relevant Context 1:\n",
      "Source: MedMCQA\n",
      "Type: expert_explanation\n",
      "Text preview: DM is a syndrome consisting of hyperglycemia, large vessel disease, micro vascular disease, and neuropathy. The classic presenting symptoms are increased thirst, polyuria, polyphagia, and weight loss....\n",
      "\n",
      "Relevant Context 2:\n",
      "Source: MedMCQA\n",
      "Type: expert_explanation\n",
      "Text preview: Manifestations of DKA are : * Symptoms - Nausea/vomiting, thirst/polyuria, abdominal pain, shoness of breath. * Physical Findings - Tachycardia, dehydration/hypotension, tachypnea/Kussmaul respiration...\n",
      "\n",
      "Relevant Context 3:\n",
      "Source: MedMCQA\n",
      "Type: expert_explanation\n",
      "Text preview: Symptoms of hyperglycemia include polyuria, polydipsia, weight loss, fatigue, weakness, blurry vision, frequent superficial infections (vaginitis, fungal skin infections), and slow healing of skin les...\n",
      "\n",
      "Query: How is breast cancer diagnosed?\n",
      "\n",
      "Relevant Context 1:\n",
      "Source: MedMCQA\n",
      "Type: expert_explanation\n",
      "Text preview: HER2 positive breast cancer is diagnosed by the IHC or FISH test. A result of HER 2 positive is impoant, as it indicates that the cancer can be treated with Herceptin, in combination with other chemot...\n",
      "\n",
      "Relevant Context 2:\n",
      "Source: MedMCQA\n",
      "Type: expert_explanation\n",
      "Text preview: Ans. C: Ductal Ductal carcinoma in situ (DCIS): DCIS, the most common type of non-invasive breast cancer, is confined to the ducts of the breast. DCIS is often first detected on mammogram as microcalc...\n",
      "\n",
      "Relevant Context 3:\n",
      "Source: MedMCQA\n",
      "Type: expert_explanation\n",
      "Text preview: Cancer of the breast in males constitutes <1% of total cases. It tends to present at a more advanced stage in men than in women, because it is often overlooked. It may easily be confused with the more...\n",
      "\n",
      "Query: What are the side effects of chemotherapy?\n",
      "\n",
      "Relevant Context 1:\n",
      "Source: MedMCQA\n",
      "Type: expert_explanation\n",
      "Text preview: Cisplatin; 'c' i.e., High dose cyclophosphamide The most common side effect of chemotherapy administration is nausea and vomiting. Antineoplastic agents may vary in their capacity to cause nausea and ...\n",
      "\n",
      "Relevant Context 2:\n",
      "Source: MedMCQA\n",
      "Type: expert_explanation\n",
      "Text preview: Side effects are unpredictable and depend on the type of chemo drug a person is using. Illness, easy bruising or bleeding, and hair loss are some of the most common side effects. Other common side eff...\n",
      "\n",
      "Relevant Context 3:\n",
      "Source: MedMCQA\n",
      "Type: expert_explanation\n",
      "Text preview: Cyclophosphamide is an alkylating agent that cross-links DNA and also inhibits DNA synthesis. Hemorrhagic cystitis and alopecia are common side effects. Cisplatin causes renal damage and neural toxici...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Define available models with their characteristics\n",
    "AVAILABLE_MODELS = {\n",
    "    \"pritamdeka/S-PubMedBert-MS-MARCO\": {\n",
    "        \"parameters\": \"110M\",\n",
    "        \"embedding_dim\": 768,\n",
    "        \"description\": \"Specialized for medical/biomedical text, fine-tuned on PubMed\"\n",
    "    },\n",
    "    \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\": {\n",
    "        \"parameters\": \"110M\",\n",
    "        \"embedding_dim\": 768,\n",
    "        \"description\": \"Trained on PubMed abstracts and full-text articles\"\n",
    "    },\n",
    "    \"gsarti/biobert-nli\": {\n",
    "        \"parameters\": \"110M\",\n",
    "        \"embedding_dim\": 768,\n",
    "        \"description\": \"BioBERT fine-tuned on NLI tasks, good for medical similarity\"\n",
    "    },\n",
    "    \"all-MiniLM-L6-v2\": {\n",
    "        \"parameters\": \"22M\",\n",
    "        \"embedding_dim\": 384,\n",
    "        \"description\": \"Fast and lightweight model, good balance of speed and performance\"\n",
    "    },\n",
    "    \"all-mpnet-base-v2\": {\n",
    "        \"parameters\": \"110M\", \n",
    "        \"embedding_dim\": 768,\n",
    "        \"description\": \"One of the best performing general models\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define available FAISS index types\n",
    "FAISS_INDEXES = {\n",
    "    \"IndexFlatL2\": {\n",
    "        \"description\": \"Exact L2 distance search. Most accurate but slower for large datasets.\",\n",
    "        \"use_case\": \"Small to medium datasets where accuracy is critical\",\n",
    "        \"recommended_size\": \"< 1M vectors\"\n",
    "    },\n",
    "    \"IndexIVFFlat\": {\n",
    "        \"description\": \"Inverted file with exact post-verification. Good balance of speed and accuracy.\",\n",
    "        \"use_case\": \"Medium to large datasets, allows approximate search\",\n",
    "        \"recommended_size\": \"1M - 10M vectors\"\n",
    "    },\n",
    "    \"IndexHNSWFlat\": {\n",
    "        \"description\": \"Hierarchical Navigable Small World graph. Very fast search with good accuracy.\",\n",
    "        \"use_case\": \"Large datasets where search speed is critical\",\n",
    "        \"recommended_size\": \"10M - 100M vectors\"\n",
    "    },\n",
    "    \"IndexLSH\": {\n",
    "        \"description\": \"Locality-Sensitive Hashing. Fast but less accurate.\",\n",
    "        \"use_case\": \"Very large datasets where approximate results are acceptable\",\n",
    "        \"recommended_size\": \"> 100M vectors\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def create_medical_knowledge_base():\n",
    "    \"\"\"Create a comprehensive medical knowledge base from multiple sources with quality filtering\"\"\"\n",
    "    knowledge_base = []\n",
    "    \n",
    "    print(\"Loading datasets...\")\n",
    "    pubmedqa = load_dataset(\"pubmed_qa\", \"pqa_labeled\")\n",
    "    medmcqa = load_dataset(\"medmcqa\")\n",
    "    \n",
    "    # Show some examples\n",
    "    print(\"\\nExample from PubMedQA:\")\n",
    "    example_pubmed = pubmedqa['train'][0]\n",
    "    # Convert context to string before slicing\n",
    "    context_preview = str(example_pubmed['context'])[:200] + \"...\"\n",
    "    pprint({\n",
    "        'question': example_pubmed['question'],\n",
    "        'context_preview': context_preview,\n",
    "        'long_answer': example_pubmed['long_answer']\n",
    "    })\n",
    "    \n",
    "    print(\"\\nExample from MedMCQA:\")\n",
    "    example_medmcqa = medmcqa['train'][0]\n",
    "    # Convert explanation to string before slicing\n",
    "    exp_preview = str(example_medmcqa['exp'])[:200] + \"...\" if example_medmcqa['exp'] else \"No explanation\"\n",
    "    pprint({\n",
    "        'question': example_medmcqa['question'],\n",
    "        'explanation_preview': exp_preview,\n",
    "        'correct_option': example_medmcqa['cop']\n",
    "    })\n",
    "    \n",
    "    # Add PubMedQA abstracts\n",
    "    for item in pubmedqa['train']:\n",
    "        # Join all context pieces into a single string\n",
    "        context_text = \" \".join(item['context']['contexts'])\n",
    "        if len(context_text.split()) >= 20:  # Length check\n",
    "            knowledge_base.append({\n",
    "                'text': context_text,\n",
    "                'source': 'PubMedQA',\n",
    "                'type': 'research_abstract',\n",
    "                'metadata': {\n",
    "                    'question': item['question'],\n",
    "                    'long_answer': item['long_answer'],\n",
    "                    'pubid': item['pubid']\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    # Add MedMCQA explanations\n",
    "    for item in medmcqa['train']:\n",
    "        if item['exp'] and len(item['exp'].split()) >= 20:  # Check if explanation exists and length\n",
    "            knowledge_base.append({\n",
    "                'text': item['exp'],\n",
    "                'source': 'MedMCQA', \n",
    "                'type': 'expert_explanation',\n",
    "                'metadata': {\n",
    "                    'question': item['question'],\n",
    "                    'correct_answer': item['cop']\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    print(f\"\\nCreated knowledge base with {len(knowledge_base)} entries\")\n",
    "    return knowledge_base\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]\n",
    "\n",
    "def build_retrieval_system(knowledge_base):\n",
    "    \"\"\"Build dense retrieval system with FAISS index\"\"\"\n",
    "     # Print available models and their info\n",
    "    print(\"\\nAvailable SentenceTransformer Models:\")\n",
    "    for model_name, info in AVAILABLE_MODELS.items():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"Parameters: {info['parameters']}\")\n",
    "        print(f\"Embedding Dimension: {info['embedding_dim']}\")\n",
    "        print(f\"Description: {info['description']}\")\n",
    "\n",
    "    # Model selection\n",
    "    selected_model = input(\"\\nEnter the name of the model you want to use (default: pritamdeka/S-PubMedBert-MS-MARCO): \").strip()\n",
    "    if not selected_model or selected_model not in AVAILABLE_MODELS:\n",
    "        print(f\"Using default model: pritamdeka/S-PubMedBert-MS-MARCO\")\n",
    "        selected_model = \"pritamdeka/S-PubMedBert-MS-MARCO\"\n",
    "\n",
    "    # Initialize encoder and configure GPU usage\n",
    "    print(f\"\\nLoading {selected_model}...\")\n",
    "    encoder = SentenceTransformer(selected_model)\n",
    "\n",
    "    # Enhanced GPU detection and configuration\n",
    "    if torch.cuda.is_available():\n",
    "        n_gpus = torch.cuda.device_count()\n",
    "        print(f\"Found {n_gpus} CUDA GPUs\")\n",
    "        \n",
    "        if n_gpus > 1:\n",
    "            print(f\"Using {n_gpus} GPUs in parallel\")\n",
    "            # Use DataParallel with all available GPUs\n",
    "            encoder = torch.nn.DataParallel(encoder)\n",
    "            # Scale batch size with number of GPUs, but cap it for stability\n",
    "            batch_size = min(32 * n_gpus, 256)  # Cap at 256 to prevent OOM\n",
    "        else:\n",
    "            print(\"Using single GPU\")\n",
    "            device = torch.device(\"cuda:0\")\n",
    "            encoder.to(device)\n",
    "            batch_size = 64\n",
    "    else:\n",
    "        print(\"CUDA is not available. Using CPU.\")\n",
    "        device = torch.device(\"cpu\")\n",
    "        batch_size = 32\n",
    "\n",
    "    # Optimize data loading for multi-GPU\n",
    "    texts = [entry['text'] for entry in knowledge_base]\n",
    "    dataset = TextDataset(texts)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=min(4, os.cpu_count() or 1),\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        persistent_workers=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "\n",
    "    # Optimized embedding generation\n",
    "    print(\"Generating embeddings...\")\n",
    "    embeddings_list = []\n",
    "    encoder.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            try:\n",
    "                if isinstance(encoder, torch.nn.DataParallel):\n",
    "                    # Automatic batch splitting across GPUs\n",
    "                    batch_embeddings = encoder.module.encode(\n",
    "                        batch,\n",
    "                        convert_to_numpy=True,\n",
    "                        device=None  # Let DataParallel handle device placement\n",
    "                    )\n",
    "                else:\n",
    "                    # Single GPU or CPU processing\n",
    "                    batch_embeddings = encoder.encode(\n",
    "                        batch,\n",
    "                        convert_to_numpy=True,\n",
    "                        device=device if 'device' in locals() else None\n",
    "                    )\n",
    "                \n",
    "                embeddings_list.append(batch_embeddings)\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error processing batch: {e}\")\n",
    "                # Graceful fallback with reduced batch size\n",
    "                if len(batch) > 1:\n",
    "                    print(\"Reducing batch size and retrying...\")\n",
    "                    # Process in smaller chunks\n",
    "                    chunk_size = len(batch) // 4\n",
    "                    for i in range(0, len(batch), chunk_size):\n",
    "                        sub_batch = batch[i:i + chunk_size]\n",
    "                        if isinstance(encoder, torch.nn.DataParallel):\n",
    "                            sub_embeddings = encoder.module.encode(\n",
    "                                sub_batch,\n",
    "                                convert_to_numpy=True,\n",
    "                                device=None\n",
    "                            )\n",
    "                        else:\n",
    "                            sub_embeddings = encoder.encode(\n",
    "                                sub_batch,\n",
    "                                convert_to_numpy=True,\n",
    "                                device=device if 'device' in locals() else None\n",
    "                            )\n",
    "                        embeddings_list.append(sub_embeddings)\n",
    "                else:\n",
    "                    raise e\n",
    "\n",
    "            # Explicit GPU memory cleanup\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    # Concatenate embeddings\n",
    "    print(\"Concatenating embeddings...\")\n",
    "    embeddings = np.concatenate(embeddings_list, axis=0)\n",
    "    \n",
    "    # FAISS index selection and creation\n",
    "    print(\"\\nAvailable FAISS Index Types:\")\n",
    "    for index_name, info in FAISS_INDEXES.items():\n",
    "        print(f\"\\n{index_name}:\")\n",
    "        print(f\"Description: {info['description']}\")\n",
    "        print(f\"Use Case: {info['use_case']}\")\n",
    "        print(f\"Recommended Dataset Size: {info['recommended_size']}\")\n",
    "\n",
    "    data_size = len(embeddings)\n",
    "    recommended_index = \"IndexFlatL2\"\n",
    "    if data_size > 100_000_000:\n",
    "        recommended_index = \"IndexLSH\"\n",
    "    elif data_size > 10_000_000:\n",
    "        recommended_index = \"IndexHNSWFlat\"\n",
    "    elif data_size > 1_000_000:\n",
    "        recommended_index = \"IndexIVFFlat\"\n",
    "\n",
    "    print(f\"\\nBased on your dataset size ({data_size:,} vectors), we recommend using: {recommended_index}\")\n",
    "    \n",
    "    selected_index = input(\"\\nEnter the name of the index type you want to use (default: recommended): \").strip()\n",
    "    if not selected_index or selected_index not in FAISS_INDEXES:\n",
    "        print(f\"Using recommended index: {recommended_index}\")\n",
    "        selected_index = recommended_index\n",
    "\n",
    "    # Build FAISS index\n",
    "    dimension = embeddings.shape[1]\n",
    "    \n",
    "    try:\n",
    "        if selected_index == \"IndexFlatL2\":\n",
    "            index = faiss.IndexFlatL2(dimension)\n",
    "        elif selected_index == \"IndexIVFFlat\":\n",
    "            nlist = min(4096, max(data_size // 30, 100))\n",
    "            quantizer = faiss.IndexFlatL2(dimension)\n",
    "            index = faiss.IndexIVFFlat(quantizer, dimension, nlist)\n",
    "            print(\"Training IVF index...\")\n",
    "            index.train(embeddings)\n",
    "        elif selected_index == \"IndexHNSWFlat\":\n",
    "            M = 32\n",
    "            index = faiss.IndexHNSWFlat(dimension, M)\n",
    "        elif selected_index == \"IndexLSH\":\n",
    "            nbits = min(64, dimension)\n",
    "            index = faiss.IndexLSH(dimension, nbits)\n",
    "\n",
    "        print(\"Adding vectors to index...\")\n",
    "        index.add(embeddings)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating FAISS index: {e}\")\n",
    "        print(\"Falling back to simple IndexFlatL2...\")\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "        index.add(embeddings)\n",
    "    \n",
    "    return index, encoder, selected_model\n",
    "\n",
    "def retrieve_contexts(query, index, encoder, knowledge_base, k=3):\n",
    "    \"\"\"Retrieve relevant contexts for a query\"\"\"\n",
    "    try:\n",
    "        # Handle both DataParallel and regular encoder cases\n",
    "        if isinstance(encoder, torch.nn.DataParallel):\n",
    "            actual_encoder = encoder.module\n",
    "        else:\n",
    "            actual_encoder = encoder\n",
    "            \n",
    "        # Move query to same device as encoder\n",
    "        device = next(actual_encoder.parameters()).device\n",
    "        \n",
    "        # Encode query\n",
    "        with torch.no_grad():\n",
    "            query_vector = actual_encoder.encode([query], convert_to_numpy=True, device=device)\n",
    "        \n",
    "        # Search index\n",
    "        distances, indices = index.search(query_vector, k)\n",
    "        \n",
    "        # Return relevant contexts with metadata and distances\n",
    "        retrieved = []\n",
    "        for idx, distance in zip(indices[0], distances[0]):\n",
    "            if 0 <= idx < len(knowledge_base):  # Validate index\n",
    "                context = knowledge_base[idx].copy()\n",
    "                context['distance'] = float(distance)\n",
    "                retrieved.append(context)\n",
    "        \n",
    "        return retrieved\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving contexts: {e}\")\n",
    "        return []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    kb_file = \"medical_knowledge_base.pkl\"\n",
    "    index_file = \"faiss_index.bin\"\n",
    "    model_name_file = \"model_name.txt\"\n",
    "    \n",
    "    try:\n",
    "        if os.path.exists(kb_file) and os.path.exists(index_file) and os.path.exists(model_name_file):\n",
    "            print(\"Loading existing knowledge base and index...\")\n",
    "            with open(kb_file, 'rb') as f:\n",
    "                kb = pickle.load(f)\n",
    "            index = faiss.read_index(index_file)\n",
    "            with open(model_name_file, 'r') as f:\n",
    "                model_name = f.read().strip()\n",
    "            encoder = SentenceTransformer(model_name)\n",
    "        else:\n",
    "            print(\"Creating new knowledge base and index...\")\n",
    "            kb = create_medical_knowledge_base()\n",
    "            index, encoder, model_name = build_retrieval_system(kb)\n",
    "            \n",
    "            print(\"Saving knowledge base, index and model name...\")\n",
    "            with open(kb_file, 'wb') as f:\n",
    "                pickle.dump(kb, f)\n",
    "            faiss.write_index(index, index_file)\n",
    "            with open(model_name_file, 'w') as f:\n",
    "                f.write(model_name)\n",
    "        \n",
    "        # Example queries\n",
    "        example_queries = [\n",
    "            \"What are the symptoms of diabetes?\",\n",
    "            \"How is breast cancer diagnosed?\",\n",
    "            \"What are the side effects of chemotherapy?\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nTesting retrieval system with example queries:\")\n",
    "        for query in example_queries:\n",
    "            print(f\"\\nQuery: {query}\")\n",
    "            relevant_contexts = retrieve_contexts(query, index, encoder, kb)\n",
    "            for i, context in enumerate(relevant_contexts, 1):\n",
    "                print(f\"\\nRelevant Context {i}:\")\n",
    "                print(f\"Source: {context['source']}\")\n",
    "                print(f\"Type: {context['type']}\")\n",
    "                print(f\"Text preview: {context['text'][:200]}...\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Use an LLM with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AutoModelForSeq2SeqGeneration' from 'transformers' (/home/azureuser/mambaforge/envs/llm/lib/python3.10/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdist\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DistributedDataParallel \u001b[38;5;28;01mas\u001b[39;00m DDP\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSeq2SeqGeneration\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrouge_score\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rouge_scorer\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'AutoModelForSeq2SeqGeneration' from 'transformers' (/home/azureuser/mambaforge/envs/llm/lib/python3.10/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM \n",
    "from rouge_score import rouge_scorer\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import nltk\n",
    "from bert_score import score\n",
    "import spacy\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define available medical QA models\n",
    "MEDICAL_QA_MODELS = {\n",
    "    \"google/flan-t5-large\": {\n",
    "        \"parameters\": \"780M\",\n",
    "        \"description\": \"Strong general-purpose model, good at following instructions\",\n",
    "        \"strengths\": \"Versatile, good at structured responses\"\n",
    "    },\n",
    "    \"GanjinZero/biomedical-flan-t5-large\": {\n",
    "        \"parameters\": \"780M\",\n",
    "        \"description\": \"FLAN-T5 fine-tuned on medical datasets\",\n",
    "        \"strengths\": \"Specialized for medical domain\"\n",
    "    },\n",
    "    \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\": {\n",
    "        \"parameters\": \"110M\",\n",
    "        \"description\": \"Trained on PubMed abstracts\",\n",
    "        \"strengths\": \"Strong medical domain knowledge\"\n",
    "    },\n",
    "    \"epfl-llm/medical-llama-7b\": {\n",
    "        \"parameters\": \"7B\",\n",
    "        \"description\": \"LLaMA fine-tuned on medical data\",\n",
    "        \"strengths\": \"Comprehensive medical knowledge\"\n",
    "    }\n",
    "}\n",
    "\n",
    "class MultiGPUConfig:\n",
    "    def __init__(self):\n",
    "        self.n_gpus = torch.cuda.device_count()\n",
    "        self.using_multi_gpu = self.n_gpus > 1\n",
    "        \n",
    "    def setup_distributed(self, rank):\n",
    "        \"\"\"Initialize distributed training\"\"\"\n",
    "        if self.using_multi_gpu:\n",
    "            dist.init_process_group(\n",
    "                backend='nccl',\n",
    "                init_method='tcp://localhost:12355',\n",
    "                world_size=self.n_gpus,\n",
    "                rank=rank\n",
    "            )\n",
    "            torch.cuda.set_device(rank)\n",
    "\n",
    "def initialize_qa_model(gpu_config, rank=0):\n",
    "    \"\"\"Initialize QA model with multi-GPU support\"\"\"\n",
    "    print(\"\\nAvailable Medical QA Models:\")\n",
    "    for model_name, info in MEDICAL_QA_MODELS.items():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"Parameters: {info['parameters']}\")\n",
    "        print(f\"Description: {info['description']}\")\n",
    "        print(f\"Strengths: {info['strengths']}\")\n",
    "\n",
    "    selected_model = input(\"\\nEnter the name of the model you want to use (default: GanjinZero/biomedical-flan-t5-large): \").strip()\n",
    "    if not selected_model or selected_model not in MEDICAL_QA_MODELS:\n",
    "        print(\"Using default model: GanjinZero/biomedical-flan-t5-large\")\n",
    "        selected_model = \"GanjinZero/biomedical-flan-t5-large\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(selected_model)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(selected_model)\n",
    "\n",
    "    if gpu_config.using_multi_gpu:\n",
    "        # Multi-GPU setup\n",
    "        device = torch.device(f'cuda:{rank}')\n",
    "        model = model.to(device)\n",
    "        model = DDP(model, device_ids=[rank])\n",
    "        print(f\"Using GPU {rank} in distributed mode\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        model = model.to(device)\n",
    "        print(\"Using single GPU\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        model = model.to(device)\n",
    "        print(\"Using MPS backend for Apple Silicon\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        model = model.to(device)\n",
    "        print(\"Using CPU\")\n",
    "    \n",
    "    return model, tokenizer, device\n",
    "\n",
    "class ParallelQualityMetrics:\n",
    "    def __init__(self, gpu_config, rank=0):\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "        self.gpu_config = gpu_config\n",
    "        self.rank = rank\n",
    "        \n",
    "        # Move BERT Score to appropriate device\n",
    "        if gpu_config.using_multi_gpu:\n",
    "            self.device = f'cuda:{rank}'\n",
    "        elif torch.cuda.is_available():\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "\n",
    "    def calculate_metrics_batch(self, batch_texts, batch_contexts=None):\n",
    "        \"\"\"Process multiple texts in parallel\"\"\"\n",
    "        if self.gpu_config.using_multi_gpu:\n",
    "            # Split batch across GPUs\n",
    "            local_batch_size = len(batch_texts) // self.gpu_config.n_gpus\n",
    "            start_idx = local_batch_size * self.rank\n",
    "            end_idx = start_idx + local_batch_size\n",
    "            \n",
    "            local_texts = batch_texts[start_idx:end_idx]\n",
    "            local_contexts = batch_contexts[start_idx:end_idx] if batch_contexts else None\n",
    "            \n",
    "            # Process local batch\n",
    "            local_results = [self.calculate_metrics(text, context=ctx) \n",
    "                           for text, ctx in zip(local_texts, local_contexts or [None] * len(local_texts))]\n",
    "            \n",
    "            # Gather results from all GPUs\n",
    "            all_results = [None] * len(batch_texts)\n",
    "            dist.all_gather_object(all_results, local_results)\n",
    "            \n",
    "            return all_results\n",
    "        else:\n",
    "            return [self.calculate_metrics(text, context=ctx) \n",
    "                   for text, ctx in zip(batch_texts, batch_contexts or [None] * len(batch_texts))]\n",
    "    \n",
    "    def calculate_metrics(self, generated_text, reference_text=None, context=None):\n",
    "        metrics = {}\n",
    "        \n",
    "        # Content relevance (if context is provided)\n",
    "        if context:\n",
    "            metrics['context_relevance'] = self._calculate_context_relevance(generated_text, context)\n",
    "        \n",
    "        # Medical entity coverage\n",
    "        metrics['medical_entities'] = self._count_medical_entities(generated_text)\n",
    "        \n",
    "        # Response length and complexity\n",
    "        metrics['response_length'] = len(generated_text.split())\n",
    "        metrics['avg_word_length'] = np.mean([len(word) for word in generated_text.split()])\n",
    "        \n",
    "        # Reference-based metrics (if reference is provided)\n",
    "        if reference_text:\n",
    "            rouge_scores = self.rouge_scorer.score(generated_text, reference_text)\n",
    "            metrics['rouge1'] = rouge_scores['rouge1'].fmeasure\n",
    "            metrics['rouge2'] = rouge_scores['rouge2'].fmeasure\n",
    "            metrics['rougeL'] = rouge_scores['rougeL'].fmeasure\n",
    "            \n",
    "            # BLEU score\n",
    "            reference_tokens = [reference_text.split()]\n",
    "            candidate_tokens = generated_text.split()\n",
    "            metrics['bleu'] = sentence_bleu(reference_tokens, candidate_tokens)\n",
    "            \n",
    "            # BERTScore\n",
    "            P, R, F1 = score([generated_text], [reference_text], lang='en', verbose=False)\n",
    "            metrics['bert_score'] = F1.mean().item()\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _calculate_context_relevance(self, text, context):\n",
    "        \"\"\"Calculate semantic similarity between generated text and context\"\"\"\n",
    "        doc1 = self.nlp(text)\n",
    "        doc2 = self.nlp(context)\n",
    "        return doc1.similarity(doc2)\n",
    "    \n",
    "    def _count_medical_entities(self, text):\n",
    "        \"\"\"Count medical entities in text using spaCy\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        medical_ents = [ent for ent in doc.ents if ent.label_ in ['DISEASE', 'CHEMICAL', 'PROCEDURE']]\n",
    "        return len(medical_ents)\n",
    "\n",
    "def parallel_rag_infer(questions, kb, index, encoder, qa_model, tokenizer, device, metrics, \n",
    "                      gpu_config, rank=0, use_context=True, top_k=3, batch_size=8):\n",
    "    \"\"\"Parallel RAG-enhanced medical QA inference\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Create batches\n",
    "    for i in range(0, len(questions), batch_size):\n",
    "        batch_questions = questions[i:i + batch_size]\n",
    "        \n",
    "        if use_context:\n",
    "            # Parallel context retrieval\n",
    "            batch_contexts = []\n",
    "            for question in batch_questions:\n",
    "                contexts = retrieve_contexts(question, index, encoder, kb, k=top_k)\n",
    "                combined_context = \"\\n\".join([ctx['text'] for ctx in contexts])\n",
    "                batch_contexts.append(combined_context)\n",
    "            \n",
    "            # Prepare prompts with context\n",
    "            prompts = [\n",
    "                f\"Answer the medical question based on the following context:\\nContext: {ctx}\\nQuestion: {q}\\nAnswer:\"\n",
    "                for q, ctx in zip(batch_questions, batch_contexts)\n",
    "            ]\n",
    "        else:\n",
    "            batch_contexts = None\n",
    "            prompts = [\n",
    "                f\"Answer the medical question:\\nQuestion: {q}\\nAnswer:\"\n",
    "                for q in batch_questions\n",
    "            ]\n",
    "        \n",
    "        # Tokenize batch\n",
    "        inputs = tokenizer(\n",
    "            prompts, \n",
    "            return_tensors=\"pt\", \n",
    "            max_length=1024, \n",
    "            truncation=True, \n",
    "            padding=True\n",
    "        ).to(device)\n",
    "        \n",
    "        # Generate answers in parallel\n",
    "        with torch.no_grad():\n",
    "            outputs = qa_model.module.generate(\n",
    "                inputs.input_ids,\n",
    "                max_length=256,\n",
    "                num_beams=4,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                early_stopping=True\n",
    "            ) if isinstance(qa_model, DDP) else qa_model.generate(\n",
    "                inputs.input_ids,\n",
    "                max_length=256,\n",
    "                num_beams=4,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            \n",
    "            batch_answers = [\n",
    "                tokenizer.decode(output, skip_special_tokens=True)\n",
    "                for output in outputs\n",
    "            ]\n",
    "        \n",
    "        # Calculate metrics in parallel\n",
    "        batch_metrics = metrics.calculate_metrics_batch(\n",
    "            batch_answers,\n",
    "            batch_contexts if use_context else None\n",
    "        )\n",
    "        \n",
    "        # Combine results\n",
    "        for q, a, m, c in zip(batch_questions, batch_answers, batch_metrics, \n",
    "                             batch_contexts if use_context else [None] * len(batch_questions)):\n",
    "            results.append({\n",
    "                'question': q,\n",
    "                'context': c,\n",
    "                'answer': a,\n",
    "                'metrics': m\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def compare_rag_performance_parallel():\n",
    "    \"\"\"Compare and evaluate RAG vs. no-RAG performance using multiple GPUs\"\"\"\n",
    "    \n",
    "    # Initialize multi-GPU setup\n",
    "    gpu_config = MultiGPUConfig()\n",
    "    \n",
    "    if gpu_config.using_multi_gpu:\n",
    "        print(f\"Using {gpu_config.n_gpus} GPUs\")\n",
    "        \n",
    "        # Launch processes for each GPU\n",
    "        torch.multiprocessing.spawn(\n",
    "            run_distributed_comparison,\n",
    "            args=(gpu_config,),\n",
    "            nprocs=gpu_config.n_gpus\n",
    "        )\n",
    "    else:\n",
    "        # Single GPU or CPU mode\n",
    "        run_distributed_comparison(0, gpu_config)\n",
    "\n",
    "def run_distributed_comparison(rank, gpu_config):\n",
    "    \"\"\"Run comparison on a single GPU in distributed setting\"\"\"\n",
    "    \n",
    "    if gpu_config.using_multi_gpu:\n",
    "        gpu_config.setup_distributed(rank)\n",
    "    \n",
    "    # Initialize components\n",
    "    qa_model, tokenizer, device = initialize_qa_model(gpu_config, rank)\n",
    "    metrics = ParallelQualityMetrics(gpu_config, rank)\n",
    "    \n",
    "    # Load knowledge base and retrieval components\n",
    "    try:\n",
    "        with open(\"medical_knowledge_base.pkl\", 'rb') as f:\n",
    "            kb = pickle.load(f)\n",
    "        index = faiss.read_index(\"faiss_index.bin\")\n",
    "        with open(\"model_name.txt\", 'r') as f:\n",
    "            model_name = f.read().strip()\n",
    "        encoder = SentenceTransformer(model_name)\n",
    "        \n",
    "        if gpu_config.using_multi_gpu:\n",
    "            encoder = encoder.to(f'cuda:{rank}')\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading knowledge base: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Test questions\n",
    "    test_questions = [\n",
    "        \"What are the early symptoms of diabetes?\",\n",
    "        \"How is rheumatoid arthritis diagnosed?\",\n",
    "        \"What are the common side effects of chemotherapy?\",\n",
    "        \"How is high blood pressure treated?\",\n",
    "        \"What causes migraine headaches?\"\n",
    "    ]\n",
    "    \n",
    "    if rank == 0:  # Only print on main process\n",
    "        print(\"\\nComparing RAG vs. No-RAG Performance:\")\n",
    "        print(\"=\" * 80)\n",
    "    \n",
    "    # Generate answers with and without RAG in parallel\n",
    "    rag_results = parallel_rag_infer(\n",
    "        test_questions, kb, index, encoder, qa_model, tokenizer, \n",
    "        device, metrics, gpu_config, rank, use_context=True\n",
    "    )\n",
    "    \n",
    "    no_rag_results = parallel_rag_infer(\n",
    "        test_questions, kb, index, encoder, qa_model, tokenizer, \n",
    "        device, metrics, gpu_config, rank, use_context=False\n",
    "    )\n",
    "    \n",
    "   if rank == 0:  # Only print results on main process\n",
    "    print(\"\\nResults with RAG:\")\n",
    "    for result in rag_results:\n",
    "        print(f\"\\nQ: {result['question']}\")\n",
    "        print(f\"A: {result['answer']}\")\n",
    "        print(f\"Metrics: {result['metrics']}\")\n",
    "        \n",
    "    print(\"\\nResults without RAG:\")\n",
    "    for result in no_rag_results:\n",
    "        print(f\"\\nQ: {result['question']}\")\n",
    "        print(f\"A: {result['answer']}\")\n",
    "        print(f\"Metrics: {result['metrics']}\")\n",
    "    \n",
    "    if gpu_config.using_multi_gpu:\n",
    "        dist.destroy_process_group()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    compare_rag_performance_parallel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning Flan-T5 with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10178/10178 [00:03<00:00, 2731.16 examples/s]\n",
      "Map: 100%|██████████| 1272/1272 [00:00<00:00, 2980.02 examples/s]\n",
      "/var/folders/7m/znbybd0d4dq84vh8my014q3c0000gn/T/ipykernel_50853/2357161580.py:59: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 8.42 GB, other allocations: 11.98 GB, max allowed: 20.40 GB). Tried to allocate 741.00 KB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 68\u001b[0m\n\u001b[1;32m     46\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     47\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./flan_t5_medical\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     48\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m     push_to_hub\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     57\u001b[0m )\n\u001b[1;32m     59\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     60\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     61\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 68\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unet_env/lib/python3.10/site-packages/transformers/trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unet_env/lib/python3.10/site-packages/transformers/trainer.py:2531\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2524\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2525\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2529\u001b[0m )\n\u001b[1;32m   2530\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2531\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2534\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2535\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2536\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2537\u001b[0m ):\n\u001b[1;32m   2538\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/unet_env/lib/python3.10/site-packages/transformers/trainer.py:3675\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3674\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3675\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3677\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3679\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3680\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3681\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/unet_env/lib/python3.10/site-packages/transformers/trainer.py:3731\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3729\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3730\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3731\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3732\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3733\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/unet_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unet_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/unet_env/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1854\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1851\u001b[0m \u001b[38;5;66;03m# Encode if needed (training, first prediction pass)\u001b[39;00m\n\u001b[1;32m   1852\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1853\u001b[0m     \u001b[38;5;66;03m# Convert encoder inputs in embeddings if needed\u001b[39;00m\n\u001b[0;32m-> 1854\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1855\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1857\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1858\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1859\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n\u001b[1;32m   1864\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[1;32m   1865\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1866\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1867\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1868\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/unet_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unet_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/unet_env/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1124\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1107\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1108\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[1;32m   1109\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1121\u001b[0m         cache_position,\n\u001b[1;32m   1122\u001b[0m     )\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1124\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/unet_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unet_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/unet_env/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:675\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    661\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    673\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    674\u001b[0m ):\n\u001b[0;32m--> 675\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    685\u001b[0m     hidden_states, past_key_value \u001b[38;5;241m=\u001b[39m self_attention_outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    686\u001b[0m     attention_outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/unet_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unet_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/unet_env/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:592\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    583\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    590\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    591\u001b[0m ):\n\u001b[0;32m--> 592\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSelfAttention(\n\u001b[1;32m    594\u001b[0m         normed_hidden_states,\n\u001b[1;32m    595\u001b[0m         mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    601\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    602\u001b[0m     )\n\u001b[1;32m    603\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/unet_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unet_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/unet_env/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:249\u001b[0m, in \u001b[0;36mT5LayerNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;66;03m# T5 uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# Square Layer Normalization https://arxiv.org/abs/1910.07467 thus varience is calculated\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;66;03m# w/o mean and there is no bias. Additionally we want to make sure that the accumulation for\u001b[39;00m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;66;03m# half-precision inputs is done in fp32\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m     variance \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    250\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrsqrt(variance \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariance_epsilon)\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;66;03m# convert into half-precision if necessary\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 8.42 GB, other allocations: 11.98 GB, max allowed: 20.40 GB). Tried to allocate 741.00 KB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "def prepare_finetune_data(df):\n",
    "    data_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        # input: RAG style prompt\n",
    "        input_text = (\n",
    "            f\"answer the medical question based on context:\\n\"\n",
    "            f\"Context: {row['context']}\\n\"\n",
    "            f\"Question: {row['question']}\\n\"\n",
    "            f\"Answer:\"\n",
    "        )\n",
    "        # target: correct answer\n",
    "        target_text = row[\"answer\"]\n",
    "        data_list.append({\"input_text\": input_text, \"target_text\": target_text})\n",
    "    return pd.DataFrame(data_list)\n",
    "\n",
    "ft_train = prepare_finetune_data(df_train_proc)\n",
    "ft_val = prepare_finetune_data(df_val_proc)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(ft_train)\n",
    "val_dataset = Dataset.from_pandas(ft_val)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    model_inputs = tokenizer(example[\"input_text\"], max_length=512, truncation=True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(example[\"target_text\"], max_length=128, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True,\n",
    "    max_length=512,\n",
    "    label_pad_token_id=-100,\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "valid_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "train_dataset = train_dataset.remove_columns([\"input_text\", \"target_text\"])\n",
    "val_dataset = val_dataset.remove_columns([\"input_text\", \"target_text\"])\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./flan_t5_medical\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs = 1,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer, \n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
