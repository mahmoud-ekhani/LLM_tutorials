{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "def preprocess_medqa(df):\n",
    "    \"\"\" Prepares dataset for retrieval-based QA. \"\"\"\n",
    "    processed_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        question = row[\"sent1\"]\n",
    "        # Four multiple-choice options\n",
    "        options = [row[f\"ending{i}\"] for i in range(4)]\n",
    "        # 'label' indicates which option is correct\n",
    "        correct_answer = options[row[\"label\"]]\n",
    "\n",
    "        processed_data.append({\n",
    "            \"question\": question,\n",
    "            \"options\": options,\n",
    "            \"answer\": correct_answer,\n",
    "            # For real RAG, you might fetch relevant doc passages or knowledge base entries. \n",
    "            # Here we use the 4 options as \"context\" for demonstration.\n",
    "            \"context\": \" \".join(options)  \n",
    "        })\n",
    "    return pd.DataFrame(processed_data)\n",
    "\n",
    "# Check if processed files already exist\n",
    "if (os.path.exists(\"medical_train.csv\") and \n",
    "    os.path.exists(\"medical_val.csv\") and \n",
    "    os.path.exists(\"medical_test.csv\")):\n",
    "    \n",
    "    # Load existing processed files\n",
    "    df_train_proc = pd.read_csv(\"medical_train.csv\")\n",
    "    df_val_proc = pd.read_csv(\"medical_val.csv\") \n",
    "    df_test_proc = pd.read_csv(\"medical_test.csv\")\n",
    "    \n",
    "else:\n",
    "    # Load and process MedQA-USMLE dataset\n",
    "    dataset = load_dataset(\"GBaker/MedQA-USMLE-4-options-hf\")\n",
    "\n",
    "    df_train = pd.DataFrame(dataset[\"train\"])\n",
    "    df_val = pd.DataFrame(dataset[\"validation\"])\n",
    "    df_test = pd.DataFrame(dataset[\"test\"])\n",
    "\n",
    "    df_train_proc = preprocess_medqa(df_train)\n",
    "    df_val_proc = preprocess_medqa(df_val)\n",
    "    df_test_proc = preprocess_medqa(df_test)\n",
    "\n",
    "    # Save processed datasets\n",
    "    df_train_proc.to_csv(\"medical_train.csv\", index=False)\n",
    "    df_val_proc.to_csv(\"medical_val.csv\", index=False)\n",
    "    df_test_proc.to_csv(\"medical_test.csv\", index=False)\n",
    "\n",
    "print(df_train_proc.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create medical knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Define available models with their characteristics\n",
    "AVAILABLE_MODELS = {\n",
    "    \"pritamdeka/S-PubMedBert-MS-MARCO\": {\n",
    "        \"parameters\": \"110M\",\n",
    "        \"embedding_dim\": 768,\n",
    "        \"description\": \"Specialized for medical/biomedical text, fine-tuned on PubMed\"\n",
    "    },\n",
    "    \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\": {\n",
    "        \"parameters\": \"110M\",\n",
    "        \"embedding_dim\": 768,\n",
    "        \"description\": \"Trained on PubMed abstracts and full-text articles\"\n",
    "    },\n",
    "    \"gsarti/biobert-nli\": {\n",
    "        \"parameters\": \"110M\",\n",
    "        \"embedding_dim\": 768,\n",
    "        \"description\": \"BioBERT fine-tuned on NLI tasks, good for medical similarity\"\n",
    "    },\n",
    "    \"all-MiniLM-L6-v2\": {\n",
    "        \"parameters\": \"22M\",\n",
    "        \"embedding_dim\": 384,\n",
    "        \"description\": \"Fast and lightweight model, good balance of speed and performance\"\n",
    "    },\n",
    "    \"all-mpnet-base-v2\": {\n",
    "        \"parameters\": \"110M\", \n",
    "        \"embedding_dim\": 768,\n",
    "        \"description\": \"One of the best performing general models\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define available FAISS index types\n",
    "FAISS_INDEXES = {\n",
    "    \"IndexFlatL2\": {\n",
    "        \"description\": \"Exact L2 distance search. Most accurate but slower for large datasets.\",\n",
    "        \"use_case\": \"Small to medium datasets where accuracy is critical\",\n",
    "        \"recommended_size\": \"< 1M vectors\"\n",
    "    },\n",
    "    \"IndexIVFFlat\": {\n",
    "        \"description\": \"Inverted file with exact post-verification. Good balance of speed and accuracy.\",\n",
    "        \"use_case\": \"Medium to large datasets, allows approximate search\",\n",
    "        \"recommended_size\": \"1M - 10M vectors\"\n",
    "    },\n",
    "    \"IndexHNSWFlat\": {\n",
    "        \"description\": \"Hierarchical Navigable Small World graph. Very fast search with good accuracy.\",\n",
    "        \"use_case\": \"Large datasets where search speed is critical\",\n",
    "        \"recommended_size\": \"10M - 100M vectors\"\n",
    "    },\n",
    "    \"IndexLSH\": {\n",
    "        \"description\": \"Locality-Sensitive Hashing. Fast but less accurate.\",\n",
    "        \"use_case\": \"Very large datasets where approximate results are acceptable\",\n",
    "        \"recommended_size\": \"> 100M vectors\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def create_medical_knowledge_base():\n",
    "    \"\"\"Create a comprehensive medical knowledge base from multiple sources with quality filtering\"\"\"\n",
    "    knowledge_base = []\n",
    "    \n",
    "    print(\"Loading datasets...\")\n",
    "    pubmedqa = load_dataset(\"pubmed_qa\", \"pqa_labeled\")\n",
    "    medmcqa = load_dataset(\"medmcqa\")\n",
    "    \n",
    "    # Show some examples\n",
    "    print(\"\\nExample from PubMedQA:\")\n",
    "    example_pubmed = pubmedqa['train'][0]\n",
    "    # Convert context to string before slicing\n",
    "    context_preview = str(example_pubmed['context'])[:200] + \"...\"\n",
    "    pprint({\n",
    "        'question': example_pubmed['question'],\n",
    "        'context_preview': context_preview,\n",
    "        'long_answer': example_pubmed['long_answer']\n",
    "    })\n",
    "    \n",
    "    print(\"\\nExample from MedMCQA:\")\n",
    "    example_medmcqa = medmcqa['train'][0]\n",
    "    # Convert explanation to string before slicing\n",
    "    exp_preview = str(example_medmcqa['exp'])[:200] + \"...\" if example_medmcqa['exp'] else \"No explanation\"\n",
    "    pprint({\n",
    "        'question': example_medmcqa['question'],\n",
    "        'explanation_preview': exp_preview,\n",
    "        'correct_option': example_medmcqa['cop']\n",
    "    })\n",
    "    \n",
    "    # Add PubMedQA abstracts\n",
    "    for item in pubmedqa['train']:\n",
    "        # Join all context pieces into a single string\n",
    "        context_text = \" \".join(item['context']['contexts'])\n",
    "        if len(context_text.split()) >= 20:  # Length check\n",
    "            knowledge_base.append({\n",
    "                'text': context_text,\n",
    "                'source': 'PubMedQA',\n",
    "                'type': 'research_abstract',\n",
    "                'metadata': {\n",
    "                    'question': item['question'],\n",
    "                    'long_answer': item['long_answer'],\n",
    "                    'pubid': item['pubid']\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    # Add MedMCQA explanations\n",
    "    for item in medmcqa['train']:\n",
    "        if item['exp'] and len(item['exp'].split()) >= 20:  # Check if explanation exists and length\n",
    "            knowledge_base.append({\n",
    "                'text': item['exp'],\n",
    "                'source': 'MedMCQA', \n",
    "                'type': 'expert_explanation',\n",
    "                'metadata': {\n",
    "                    'question': item['question'],\n",
    "                    'correct_answer': item['cop']\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    print(f\"\\nCreated knowledge base with {len(knowledge_base)} entries\")\n",
    "    return knowledge_base\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]\n",
    "\n",
    "def build_retrieval_system(knowledge_base):\n",
    "    \"\"\"Build dense retrieval system with FAISS index\"\"\"\n",
    "     # Print available models and their info\n",
    "    print(\"\\nAvailable SentenceTransformer Models:\")\n",
    "    for model_name, info in AVAILABLE_MODELS.items():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"Parameters: {info['parameters']}\")\n",
    "        print(f\"Embedding Dimension: {info['embedding_dim']}\")\n",
    "        print(f\"Description: {info['description']}\")\n",
    "\n",
    "    # Model selection\n",
    "    selected_model = input(\"\\nEnter the name of the model you want to use (default: pritamdeka/S-PubMedBert-MS-MARCO): \").strip()\n",
    "    if not selected_model or selected_model not in AVAILABLE_MODELS:\n",
    "        print(f\"Using default model: pritamdeka/S-PubMedBert-MS-MARCO\")\n",
    "        selected_model = \"pritamdeka/S-PubMedBert-MS-MARCO\"\n",
    "\n",
    "    # Initialize encoder and configure GPU usage\n",
    "    print(f\"\\nLoading {selected_model}...\")\n",
    "    encoder = SentenceTransformer(selected_model)\n",
    "\n",
    "    # Enhanced GPU detection and configuration\n",
    "    if torch.cuda.is_available():\n",
    "        n_gpus = torch.cuda.device_count()\n",
    "        print(f\"Found {n_gpus} CUDA GPUs\")\n",
    "        \n",
    "        if n_gpus > 1:\n",
    "            print(f\"Using {n_gpus} GPUs in parallel\")\n",
    "            # Use DataParallel with all available GPUs\n",
    "            encoder = torch.nn.DataParallel(encoder)\n",
    "            # Scale batch size with number of GPUs, but cap it for stability\n",
    "            batch_size = min(32 * n_gpus, 256)  # Cap at 256 to prevent OOM\n",
    "        else:\n",
    "            print(\"Using single GPU\")\n",
    "            device = torch.device(\"cuda:0\")\n",
    "            encoder.to(device)\n",
    "            batch_size = 64\n",
    "    else:\n",
    "        print(\"CUDA is not available. Using CPU.\")\n",
    "        device = torch.device(\"cpu\")\n",
    "        batch_size = 32\n",
    "\n",
    "    # Optimize data loading for multi-GPU\n",
    "    texts = [entry['text'] for entry in knowledge_base]\n",
    "    dataset = TextDataset(texts)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=min(4, os.cpu_count() or 1),\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        persistent_workers=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "\n",
    "    # Optimized embedding generation\n",
    "    print(\"Generating embeddings...\")\n",
    "    embeddings_list = []\n",
    "    encoder.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            try:\n",
    "                if isinstance(encoder, torch.nn.DataParallel):\n",
    "                    # Automatic batch splitting across GPUs\n",
    "                    batch_embeddings = encoder.module.encode(\n",
    "                        batch,\n",
    "                        convert_to_numpy=True,\n",
    "                        device=None  # Let DataParallel handle device placement\n",
    "                    )\n",
    "                else:\n",
    "                    # Single GPU or CPU processing\n",
    "                    batch_embeddings = encoder.encode(\n",
    "                        batch,\n",
    "                        convert_to_numpy=True,\n",
    "                        device=device if 'device' in locals() else None\n",
    "                    )\n",
    "                \n",
    "                embeddings_list.append(batch_embeddings)\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error processing batch: {e}\")\n",
    "                # Graceful fallback with reduced batch size\n",
    "                if len(batch) > 1:\n",
    "                    print(\"Reducing batch size and retrying...\")\n",
    "                    # Process in smaller chunks\n",
    "                    chunk_size = len(batch) // 4\n",
    "                    for i in range(0, len(batch), chunk_size):\n",
    "                        sub_batch = batch[i:i + chunk_size]\n",
    "                        if isinstance(encoder, torch.nn.DataParallel):\n",
    "                            sub_embeddings = encoder.module.encode(\n",
    "                                sub_batch,\n",
    "                                convert_to_numpy=True,\n",
    "                                device=None\n",
    "                            )\n",
    "                        else:\n",
    "                            sub_embeddings = encoder.encode(\n",
    "                                sub_batch,\n",
    "                                convert_to_numpy=True,\n",
    "                                device=device if 'device' in locals() else None\n",
    "                            )\n",
    "                        embeddings_list.append(sub_embeddings)\n",
    "                else:\n",
    "                    raise e\n",
    "\n",
    "            # Explicit GPU memory cleanup\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    # Concatenate embeddings\n",
    "    print(\"Concatenating embeddings...\")\n",
    "    embeddings = np.concatenate(embeddings_list, axis=0)\n",
    "    \n",
    "    # FAISS index selection and creation\n",
    "    print(\"\\nAvailable FAISS Index Types:\")\n",
    "    for index_name, info in FAISS_INDEXES.items():\n",
    "        print(f\"\\n{index_name}:\")\n",
    "        print(f\"Description: {info['description']}\")\n",
    "        print(f\"Use Case: {info['use_case']}\")\n",
    "        print(f\"Recommended Dataset Size: {info['recommended_size']}\")\n",
    "\n",
    "    data_size = len(embeddings)\n",
    "    recommended_index = \"IndexFlatL2\"\n",
    "    if data_size > 100_000_000:\n",
    "        recommended_index = \"IndexLSH\"\n",
    "    elif data_size > 10_000_000:\n",
    "        recommended_index = \"IndexHNSWFlat\"\n",
    "    elif data_size > 1_000_000:\n",
    "        recommended_index = \"IndexIVFFlat\"\n",
    "\n",
    "    print(f\"\\nBased on your dataset size ({data_size:,} vectors), we recommend using: {recommended_index}\")\n",
    "    \n",
    "    selected_index = input(\"\\nEnter the name of the index type you want to use (default: recommended): \").strip()\n",
    "    if not selected_index or selected_index not in FAISS_INDEXES:\n",
    "        print(f\"Using recommended index: {recommended_index}\")\n",
    "        selected_index = recommended_index\n",
    "\n",
    "    # Build FAISS index\n",
    "    dimension = embeddings.shape[1]\n",
    "    \n",
    "    try:\n",
    "        if selected_index == \"IndexFlatL2\":\n",
    "            index = faiss.IndexFlatL2(dimension)\n",
    "        elif selected_index == \"IndexIVFFlat\":\n",
    "            nlist = min(4096, max(data_size // 30, 100))\n",
    "            quantizer = faiss.IndexFlatL2(dimension)\n",
    "            index = faiss.IndexIVFFlat(quantizer, dimension, nlist)\n",
    "            print(\"Training IVF index...\")\n",
    "            index.train(embeddings)\n",
    "        elif selected_index == \"IndexHNSWFlat\":\n",
    "            M = 32\n",
    "            index = faiss.IndexHNSWFlat(dimension, M)\n",
    "        elif selected_index == \"IndexLSH\":\n",
    "            nbits = min(64, dimension)\n",
    "            index = faiss.IndexLSH(dimension, nbits)\n",
    "\n",
    "        print(\"Adding vectors to index...\")\n",
    "        index.add(embeddings)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating FAISS index: {e}\")\n",
    "        print(\"Falling back to simple IndexFlatL2...\")\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "        index.add(embeddings)\n",
    "    \n",
    "    return index, encoder, selected_model\n",
    "\n",
    "def retrieve_contexts(query, index, encoder, knowledge_base, k=3):\n",
    "    \"\"\"Retrieve relevant contexts for a query\"\"\"\n",
    "    try:\n",
    "        # Handle both DataParallel and regular encoder cases\n",
    "        if isinstance(encoder, torch.nn.DataParallel):\n",
    "            actual_encoder = encoder.module\n",
    "        else:\n",
    "            actual_encoder = encoder\n",
    "            \n",
    "        # Move query to same device as encoder\n",
    "        device = next(actual_encoder.parameters()).device\n",
    "        \n",
    "        # Encode query\n",
    "        with torch.no_grad():\n",
    "            query_vector = actual_encoder.encode([query], convert_to_numpy=True, device=device)\n",
    "        \n",
    "        # Search index\n",
    "        distances, indices = index.search(query_vector, k)\n",
    "        \n",
    "        # Return relevant contexts with metadata and distances\n",
    "        retrieved = []\n",
    "        for idx, distance in zip(indices[0], distances[0]):\n",
    "            if 0 <= idx < len(knowledge_base):  # Validate index\n",
    "                context = knowledge_base[idx].copy()\n",
    "                context['distance'] = float(distance)\n",
    "                retrieved.append(context)\n",
    "        \n",
    "        return retrieved\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving contexts: {e}\")\n",
    "        return []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    kb_file = \"medical_knowledge_base.pkl\"\n",
    "    index_file = \"faiss_index.bin\"\n",
    "    model_name_file = \"model_name.txt\"\n",
    "    \n",
    "    try:\n",
    "        if os.path.exists(kb_file) and os.path.exists(index_file) and os.path.exists(model_name_file):\n",
    "            print(\"Loading existing knowledge base and index...\")\n",
    "            with open(kb_file, 'rb') as f:\n",
    "                kb = pickle.load(f)\n",
    "            index = faiss.read_index(index_file)\n",
    "            with open(model_name_file, 'r') as f:\n",
    "                model_name = f.read().strip()\n",
    "            encoder = SentenceTransformer(model_name)\n",
    "        else:\n",
    "            print(\"Creating new knowledge base and index...\")\n",
    "            kb = create_medical_knowledge_base()\n",
    "            index, encoder, model_name = build_retrieval_system(kb)\n",
    "            \n",
    "            print(\"Saving knowledge base, index and model name...\")\n",
    "            with open(kb_file, 'wb') as f:\n",
    "                pickle.dump(kb, f)\n",
    "            faiss.write_index(index, index_file)\n",
    "            with open(model_name_file, 'w') as f:\n",
    "                f.write(model_name)\n",
    "        \n",
    "        # Example queries\n",
    "        example_queries = [\n",
    "            \"What are the symptoms of diabetes?\",\n",
    "            \"How is breast cancer diagnosed?\",\n",
    "            \"What are the side effects of chemotherapy?\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nTesting retrieval system with example queries:\")\n",
    "        for query in example_queries:\n",
    "            print(f\"\\nQuery: {query}\")\n",
    "            relevant_contexts = retrieve_contexts(query, index, encoder, kb)\n",
    "            for i, context in enumerate(relevant_contexts, 1):\n",
    "                print(f\"\\nRelevant Context {i}:\")\n",
    "                print(f\"Source: {context['source']}\")\n",
    "                print(f\"Type: {context['type']}\")\n",
    "                print(f\"Text preview: {context['text'][:200]}...\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Use an LLM with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import medical_qa_distributed\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Optionally specify a model name, or use default\n",
    "    model_name = \"google/flan-t5-large\"  # or any other model from MEDICAL_QA_MODELS\n",
    "    \n",
    "    try:\n",
    "        medical_qa_distributed.compare_rag_performance_parallel(model_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        # If distributed fails, try single GPU mode\n",
    "        gpu_config = medical_qa_distributed.MultiGPUConfig()\n",
    "        medical_qa_distributed.run_distributed_comparison(0, gpu_config, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4:Fine-Tuning Flan-T5 with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "def prepare_finetune_data(df):\n",
    "    data_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        # input: RAG style prompt\n",
    "        input_text = (\n",
    "            f\"answer the medical question based on context:\\n\"\n",
    "            f\"Context: {row['context']}\\n\"\n",
    "            f\"Question: {row['question']}\\n\"\n",
    "            f\"Answer:\"\n",
    "        )\n",
    "        # target: correct answer\n",
    "        target_text = row[\"answer\"]\n",
    "        data_list.append({\"input_text\": input_text, \"target_text\": target_text})\n",
    "    return pd.DataFrame(data_list)\n",
    "\n",
    "ft_train = prepare_finetune_data(df_train_proc)\n",
    "ft_val = prepare_finetune_data(df_val_proc)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(ft_train)\n",
    "val_dataset = Dataset.from_pandas(ft_val)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    model_inputs = tokenizer(example[\"input_text\"], max_length=512, truncation=True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(example[\"target_text\"], max_length=128, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True,\n",
    "    max_length=512,\n",
    "    label_pad_token_id=-100,\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "valid_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "train_dataset = train_dataset.remove_columns([\"input_text\", \"target_text\"])\n",
    "val_dataset = val_dataset.remove_columns([\"input_text\", \"target_text\"])\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./flan_t5_medical\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs = 1,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer, \n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
